{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyqMV6Leq2AGWIUqyAzTQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VCHULANI15/financial_news_sentiment_analysis/blob/main/WSJ_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_headlines_for_page(page_number):\n",
        "    # Construct the URL for the current page\n",
        "    url = f'https://www.wsj.com/news/archive/2016/04/21?page={page_number}'\n",
        "\n",
        "    # Fetch the content of the page\n",
        "    response = requests.get(url)\n",
        "    # Proceed only if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        page_headlines = []\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Inspect the WSJ webpage and replace 'article-class', 'subtitle-class', and 'headline-class'\n",
        "        # with the actual class names used for the articles, subtitles, and headlines respectively\n",
        "        articles = soup.find_all('div', class_='article-class')  # Placeholder class name\n",
        "        for article in articles:\n",
        "            try:\n",
        "                subtitle = article.find('p', class_='subtitle-class').text.strip()  # Placeholder class name\n",
        "                if subtitle in ['Markets', 'Business', 'Tech', 'Earnings']:\n",
        "                    headline = article.find('h3', class_='headline-class').text.strip()  # Placeholder class name\n",
        "                    page_headlines.append(headline)\n",
        "            except AttributeError:\n",
        "                # If there's an error finding the subtitle or headline, continue to the next article\n",
        "                continue\n",
        "        return page_headlines\n",
        "    else:\n",
        "        print(f\"Failed to retrieve page {page_number} with status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "all_headlines = []\n",
        "for page_number in range(2, 8):\n",
        "    all_headlines.extend(fetch_headlines_for_page(page_number))\n",
        "\n",
        "# Print the aggregated headlines\n",
        "for headline in all_headlines:\n",
        "    print(headline)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aglyyZC_ie0T",
        "outputId": "0357c2f8-33e6-4790-c9e6-d5961ec8c82d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve page 2 with status code: 403\n",
            "Failed to retrieve page 3 with status code: 403\n",
            "Failed to retrieve page 4 with status code: 403\n",
            "Failed to retrieve page 5 with status code: 403\n",
            "Failed to retrieve page 6 with status code: 403\n",
            "Failed to retrieve page 7 with status code: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_headlines_for_page(page_number):\n",
        "    url = f'https://www.wsj.com/news/archive/2016/04/21?page={page_number}'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Link is {url}\")\n",
        "        print(f\"Failed to retrieve page {page_number}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "eX939Ax0kBCr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_headlines = []\n",
        "for page_number in range(2, 8):\n",
        "    all_headlines.extend(fetch_headlines_for_page(page_number))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP_hHz5rlA63",
        "outputId": "ba0d9ef5-0ef2-4ea0-b022-b035a8d641dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Link is https://www.wsj.com/news/archive/2016/04/21?page=2\n",
            "Failed to retrieve page 2\n",
            "Link is https://www.wsj.com/news/archive/2016/04/21?page=3\n",
            "Failed to retrieve page 3\n",
            "Link is https://www.wsj.com/news/archive/2016/04/21?page=4\n",
            "Failed to retrieve page 4\n",
            "Link is https://www.wsj.com/news/archive/2016/04/21?page=5\n",
            "Failed to retrieve page 5\n",
            "Link is https://www.wsj.com/news/archive/2016/04/21?page=6\n",
            "Failed to retrieve page 6\n",
            "Link is https://www.wsj.com/news/archive/2016/04/21?page=7\n",
            "Failed to retrieve page 7\n"
          ]
        }
      ]
    }
  ]
}